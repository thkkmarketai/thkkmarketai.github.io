---
layout: default
title: thkk market AI
description: "データ処理 / Data Preprocessing"
---

## **データ処理 / Data Preprocessing**

- リポジトリ / Repository
 - [Data Preprocessing](https://thkkmarketai.github.io/datapreprocessing)

　出所の良し悪しに関わらず、取得したデータは一連の処理プロセスを通すべきでしょう。その目的は、何も異常や欠損を探し出すだけではなく、その全体像を掴むことも兼ねているからです。

　「より良いデータ処理とは？」という議論は単純であるが故に奥深そうなので余分に首を突っ込まず、ここでは抑えるべきプロセスを幾つか述べるに留めようと思います。プロジェクトの性質上、データは金融市場の時系列データのみを扱います。

&emsp;

#### **1. 欠損値、外れ値、誤データの抽出**

- 欠損値　　・・・
- 異常値・外れ値　　・・・
- 誤データ　・・・

&emsp;

#### **2. 統計的な特性と傾向の把握**

- 統計的な特性と傾向
  - []

- 1のデータ修正
  - []

&emsp;

#### **3. 分析目的や投入モデルに合わせたデータの尺度変更（scaling）**

- Min-Max scaling
 - []
- Standardization
 - []
- Normalization
 - []
- Binning
 - []

&emsp;

　You should make regular preprocess for data regardless of its source. The reason is not only to find some errors or nulls but to get the whole picture.

　"What is the best way to preprocess?"...this is a simple question but difficult to find the answer that everybody agrees. In this section, I just introduce some important processes to do. Note that only financial time series data are covered in this project.

#### **1. extract null, outlier, and error**

- null　　・・・
- outlier　　・・・
- error　・・・

#### **2. comprehend statistical point of view**

- statistical point of view
  - []
- replace the data of null, outlier and error
  - []

#### **3. change the data scale for your analysis or AI model（scaling）**

- Min-Max scaling
  - []
- Standardization
  - []
- Normalization
  - []
- Binning
  - []

&emsp;

## **resent posts**
1. []
2. []
3. []
4. []
5. []

&emsp;
